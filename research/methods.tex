\chapter{Methods}
\label{ch:methods}

\section{Research design}
\label{sec:researchdesign}

Research questions~\ref{benefit}\ref{effectiveness}, \ref{efficiency}, \ref{perception}\ref{usefulness} and \ref{ease} will be investigated using intervention-based research. Because of the systems being used for self-study by the students, they can be individually assigned to a condition, and this enables the use of a true experimental design. Since this will provide the most valid and reliable results, this research design is implemented in this experiment.

Additionally, research questions~\ref{usefulness} and \ref{ease} will also be investigated using open questions on the questionnaire form and by conducting interviews with a sample of the participants.

Finally, research question~\ref{howused} will be investigating both quantitatively by logging the user behaviour during the experiment and by the open questions and interviews also used for research questions~\ref{usefulness} and \ref{ease}.

The quantitative and qualitative results will be mixed for the purposes of triangulation and expansion as described by \citeA{mixedmethods}. The interviews and logs could provide insight in the degree of which the systems were used the intended way and in why students had certain perceptions on using the systems. Both triangulation and expansion will be on a partial level of mixing, will take place concurrently, and the quantitative data will be dominant, since the qualitative data exists only to triangulate and expand the quantitative data. 

\section{Respondents}
\label{sec:respondents}

100 15 to 17 year old tenth grade Dutch high school students will be approached. They already have to prepare themselves for an exam on the same topic and thereby have incentive to learn. To increase the response rate, the students will be rewarded with a \euro{} 5 voucher for participation. The participants will be assigned to either the flashcard or the flashmap condition at random when they create a user account within the webapplication.

\section{Procedure}
\label{sec:procedure}

\paragraph{Review concept map, flashcards, and item bank} In order to verify whether the content offered within the system is in alignment with the learning goals set by the teachers, one of the teachers is asked to review the content. The feedback received from the teacher is afterwards incorporated by altering the dataset. This can be seen as the focus evaluation of the product \cite{slo}.

\paragraph{Approval ethical committee} Before the actual experiment can take place, the research setup first has to be approved by the ethical committee of the University of Twente. This takes place before the system is introduced to the students.

\paragraph{Presentation} Within the school curriculum there are two instructions planned for the topic of Dutch renaissance literature. At the end of the instruction, the researcher introduces the experiment and the system to the participants. This is meant both to attract students to participate, but also to provide a briefing next to the written briefing. Within the presentation, the benefits are stated (better preparation for the exam, preview for the exam), it is stressed that participation is voluntary and that the data will be collected anonymously, the informed consent form will be introduced, and finally the reward (icecream vourchers) will be announced, together with the conditions for receiving the reward. Information with regards to the seperate experimental conditions will be limited to the researcher explaining that there are two different versions, without elaborating what these versions entail. This is in order to prevent prejudgements from within the user, making it a double-blind experiment. It could however of course happen that the participants learn about each others version during the experiment, and unfortunately this cannot be prevented.

\paragraph{Informed consent form} The informed consent form, included within the \nameref{app:consentform} appendix on page~\pageref{app:consentform}, contains a letter, the written briefing, and a form which has to be signed by both the parent or caretaker and the participant. It also contains a code with which it can be verified that a user within the system did indeed sign this form. The briefing contains a description of the research, the advantages of participation, and the procedure of the experiment.

\paragraph{Division of respondents} Users will be assigned alternately to the control group or the experimental group. This pseudo-random assignment increases the validity of the experimental design without risking one group becoming larger than the other group. This however only holds up for the initial group, because dropout rates between the group could vary resulting in differently sized finished groups.

\paragraph{Descriptives} Before the experiment itself starts, the gender and the birthdate of the participants are prompted. This provides descriptive statistics necessary for the measure of generalisability of the results.

\paragraph{Pretest} Another form prompted towards the user before the start of the experiment is the pretest, measuring how much the user already knows and understands about the subject. This test is elaborated within the next section.

\paragraph{Experiment} The experiment itself consists of the participants having to review instances for 15 minutes over the course of 6 days. The 15 minutes are estimated to be the amount of time necessary to review one section in the instructional material, and the 6 days are chosen as a balance between having covered a large enough portion of the material to measure a significant learning gain without the participant investment being too large resulting in no student wanting to participate. The 6 days of learning ideally take place subsequently, since then students have a higher chance of retrieving repeating instances correctly. However, it is likely that participants could forget about the experiment or be too busy to invest the 15 minutes, and therefore they are allowed one non active day during the experiment. Each session consists of being presented by questions or incomplete concept maps, trying to retrieve the correct answer or missing concepts from memory, and indicating whether the correct answer or missing concept was successfully retrieved.

\paragraph{Posttest} The seventh day the participant logs in to the system he is prompted with the posttest in order to measure the level of knowledge and comprehension after the experiment. This test uses the same itembank as the pretest, and is thereby also elaborated within the next section.

\paragraph{Questionnaire} After filling in the posttest, the participant is also asked to fill in a questionnaire based on the Technology Acceptance Model, which is further elaborated within the next section. The form also contains two open questions, namely to describe what the participant thought was good about the system, and what he thought could be improved. Finally, he can fill in his email address if he is interested in being interviewed afterwards.

\paragraph{Debriefing} Finally, the participant is presented with the debriefing text from figure~\ref{fig:ui_debriefing} on page~\pageref{fig:ui_debriefing}. This states that the user will soon receive the voucher, that he is allowed to keep using the system, and that he can contact the researcher if he has questions or when he wants to see his personal data. The participant is now finished with the main experiment.

\paragraph{Scoring sample items} After all the results have been gathered, a small sample of the responses are reviewed by both one of the teachers and the researcher in order to establish an inter-rater reliability. This will take place after the school test itself, but before the teacher has scored the test administered by the school itself to remain unbiased. Furthermore, the sample will be a random anonymous selection of responses in order to minimise any halo effect. Finally, the samples are filtered on non-empty items which do also not exactly correspond to the response model, since these can be automatically scored. If the inter-rater reliability is too low, the scoring rubics will be altered in order to differentiate better among correct or incorrect responses, and the procedure is repeated. Otherwise, the rest of the responses is scored by the researcher.

\paragraph{Interview} Those who volunteered for the interview will be sent an invitation by email for an open group interview, also elaborated in the next section.

\paragraph{Icecream vouchers} The vouchers and the list containing the names of students having participated within the experiment will be handed over to the students after the scoring of the test administered within the school in order to avoid influencing the teachers during the marking of the tests.

\section{Instrumentation}
\label{sec:instrumentation}

\subsection{Test}

A test (either pre- or posttest) consists out of a knowledge section and a comprehension section, derived from Bloom's Taxonomy \cite{bloom}. The knowledge section aimed at measuring whether the rote memorisation was effective, and the comprehension section served the purpose of measuring whether the explicit relations within the flashmaps scaffolded the comprehension. Only these two levels were chosen, since higher levels are more time consuming to create and to answer by the students, and the first step is to only measure whether students can already generalise from only rehearsing questions on the knowledge level to questions on the comprehension level.

The itembank used for the knowledge questions were the flashcards themselves, and the itembank for the comprehension questions a seperately constructed set of 10 items. Both the knowledge questions and the comprehension questions are based on a concept map encompassing the instructional material created by the researcher. The pretest and the posttest both randomly select 5 questions from each bank with non-overlapping items. By randomly selecting the questions, the overall knowledge and comprehension are measured instead of specific subsets measured seperately on the pretest and posttest. This eliminates the specific item difficulty variable from the learning gain, increasing its validity. The random variable however also increases the variability of scores, decreasing the significance when comparing the conditions.

Finally, a rubic is created in order to quantisise the openly formulated answers, consisting out of possible answer categories for each item.

\subsection{Questionnaire}

The questionnaire consists out of items based on the Technology Acceptance Model (TAM) \cite{tam}, containing items measuring perceived usefulness and perceived ease of use. \citeA{devellis} describes that the validity of a questionnaire can be increased by formulating the items mixed positively and negatively, and by repeating the items. Therefore, the items from TAM are translated to Dutch in both a positive and a negative phrasing. For each participant, 2 sets of items are created per TAM category: for each item, a phrasing is selected at random after which the set is shuffled; another set of shuffled perceived usefulness items is created with each item having the opposite phrasing of its counterpart in set 1. This results in 4 sets of items encompassing both phrasings of all items. For each item, the participant can indicate whether he completely disagrees (-2), he disagrees (-1), he neither agrees nor disagrees (0), he agrees (1), or he completely agrees (2).

\subsection{Data collection during experiment}

During the experiment itself more data is created in order to provide statistics about the usage of the system. These statistics are mainly contained within the Response objects in the database.

\paragraph{Correctness retrievals} For each response within an instance, information is stored about whether the instance was retrieved correctly by the participant. This is not only useful for the rescheduling of the instance, but also in order to gain more insights in how often a participant was able to retrieve instances correctly, and to verify whether the success rate is indeed around 90\% as stated by \citeA{microlearning}.

\paragraph{Retrieval time} Furthermore, the start time (when an instance is sent from the server to the participant) and the end time (when the response was received by the server) are stored for each response in order to measure how much time was spent on each instance, and on the system in general.

\paragraph{Active days} Finally, a list is stored for each participant containing the dates that the participant was active. An active day is here defined as a day where the participant is active for 15 minutes or where the participant reviewed all of the available instances within the read sections. This data is useful for the system to know when to prompt the posttest, but also serves as a statistic on participation rates.

\subsection{Interview}

The interview is open instead of structured or semi-structured, since the interview should provide the interviewees the opportunity to tell about their own experiences from using the system, and since the data covering specific questions is already gathered within the usage data, the questionnaire and the two open questions. The researcher will use topics based on TAM in order to verify during the interview whether the main area is covered, with an open question at the end asking for general possible improvements. Furthermore, the interview will take place as one group interview, since the school organisation only has limited availability options for providing seperate rooms fit for interviews and the group interview being the most efficient option. At the start of the interview, the interviewees will be disclosed about the aims of the research in order to establish the different versions. Results will however not be disclosed until after the experiment in order to leave the interviewees unbiased in their opinion about the perceived usefulness. Finally, with the interviewees permission, the interview is recorded for later analysis. If not, the researcher takes notes during the interview.

\section{Analysis}
\label{sec:analysis}

\subsection{Inter-rater reliability}

In order to verify whether the rubics is a reliable measure for quantificating the open answers on the test questions, first the inter-reliability is calculated. This is done by calculating both the proportionate agreement as the more reliable weighted Cohen's kappa \cite{kappa} between of one of the teachers and the researcher which marked 10 randomly seleceted knowledge items and 12 randomly selected comprehension items. The items were selected from both the pretest and posttest from random participants, and shuffled in order to minimise any halo effect.

\subsection{Test scores}

In order to determine the score on a test, an item matrix is created with the response categories from the rubic as columns, the participants as rows, and the cells containing a 0 or 1 indicating whether the participants answer contained the response category. Each possible answer category from the rubic is chosen for the columns over the test items in order to more accurately determine item difficulties. The raw item difficulties are then calculated by the sum over the columns, whereas the raw person abilities are determined by the sum over the rows. Furthermore, the absolute and relative learning gain are calculated for comparing the conditions. The absolute learning gain entails simply subtracting the pretest person abilities from the posttest person abilities, resulting in a list of scores indicating how many more points each person scored on the posttest than on the pretest. The relative learning gain is calculated by the posttest score minus the pretest score divided by the maximum possible posttest score minus the pretest score. The main advantage of using relative gain over absolute gain is that each user can have a different maximum test score because of the items having different maximum scores and them being randomly selected for either the pre- or posttest, and the relative learning gain implicitly controls for this where the absolute gain does not.

For each subcategory, two histograms are plotted: one for the item difficulty (expressed as the average amount of correct answer on an item per person) and one for the person ability (expressed as the average score of a person divided by the amount of items). The correction is necessary in order to compare the two groups on an equal level, and the numbers are not indicatory of the percentage of correct answers on the test. Both the pretest and the posttest histograms are plotted in one graph for each condition, and the learning gains of both conditions are also plotted in one graph for visualising the differences.

\subsection{Participation statistics}

For determining the participation rates, a matrix is constructed containing the different days of the experiment as columns, the participants as rows, and the cells containing a 0 or 1 indicating the participant being active on that day. The participation rates are then calculated by the sum over the rows, and the amount of active participants from the sum over the columns. These matrixes are constructed seperately for the control group, the experimental group, and the group of combined experimental conditions. The statistics are depicted as histograms for the amount of active days for a participant, and as a scatterdiagram showing the amount of activity for each day of the experiment, combined with a step diagram showing the cumulative amount of finished participants. The days are indicated by a number from 0 to 28, where 0 is the day of presentation, day 21 is the day of the school test, and 28 is the last day with any activity.

\subsection{Instance statistics}

This category contains all statistics derived from the Instance and Response objects in the database collected during the experiment. For all of the below subcategories, item matrices are constructed for the control group, experimental group, and the group of participants from combined conditions. These matrices consist of the Flashcards or Edges in the columns, depending on the condition, and the participants in the columns. Within the combined conditions group, flashcards are mapped to the first edge in their list of sources. Furthermore, for in the number of responses and time spent categories, the double flashmap instances are removed. These are the instances presented together in one flashmap to the user. Like in the previous categories, item scores are again calculated by sum over the columns, and person scores by sum over the rows. Additionally, a relative score is calculated by dividing sum by the amount of entries: for the item score the relative score is the sum (absolute score) divided by the amount of participants, and for the person score it is the absolute score divided by the amount of items. Finally, mean scores are calculated for both rows and columns. The relative score is meant for being able to compare both groups, whereas the mean score provides additional insight in the score per reviewed instance. Finally, histograms are depicted for both the item scores as the user scores.

\paragraph{Reviewed instances} In order to determine how much of the instructional material is reviewed by each participant, the first subcategory investigated is the amount of reviewed instances. The cell of the item matrices contains whether an instance for a participant contains any response. The relative score proves here the most useful, since this is the ratio of covered material.

\paragraph{Number of responses} This statistic contains information on the number of times an instance is retrieved by a participant, which is included in order to compare whether both groups had an equal amount of opportunities to rehearse the instances. Since the ratio of retrieved instances is already known from the previous category, the mean number of responses per instance is the most useful score for comparing the experimental groups.

\paragraph{Exponents} The exponent statistic is directly derived from the get\_expontent() function from Instance, indicating the amount of responses since the last incorrect retrieval. This is included as a measure of learning progress of an instance. For the same reasons as for the previous category, the mean values are the most useful measures for comparing the groups.

\paragraph{Responses marked as correct} The ratio of the number of correct retrievals and the total number of retrievals is included in order to establish whether both groups had the same difficulty of retrieval from memory, and to verify whether participants indeed retrieve the instance correctly for 90\% of the retrievals as indicated by \citeA{microlearning}. The only relevant values are here the mean values, for the other values do not make sense.

\paragraph{Time spent} Finally, the amount of time spent is calculated for each instance by the sum the retrieval time of each response, where the retrieval time is calculated by subtracting the time the instance was sent to the client from the time the instance was received on the server. The reason why this is chosen as the heuristic for time measurement is that this is the most intensive use of the system, whereas the times in between measure only the validating of an retieved answer or an intermission. Furthermore, the responses are filtered on only having retrieval times shorter than 5 minutes, since any longer retrieval time probably indicates an intermission between learning. For the same reason, the session time is also an unreliable measure. The absolute and mean values are here the most insightful, since the absolute values provide a measure of how much time the average user spent in total on the system, and the mean values are useful for comparing an individual flashcard with a flashmap. 

\subsection{Questionnaire scores}

The last quantitative category contains the scores on the questionnaire, divided in the perceived usefulness score and the perceived ease of use score. Again, an item matrix is constructed, with the questionnaire items (either usefulness or ease of use) represented by the columns and the participants by the rows, and the cells containing how the participant rated the item. This score is determined by the rating from the positive phrasing minus the rating from the negative rating, which should result in twice the positive rating if the participant rated the negatively phrased item with the inverse rating of the positive phrased item. This result thereby ranges from -4 to 4.

\subsection{Classical test theory}

In order to establish Cronbach's alpha, classical test analyses are conducted over all of the above described statistics, using the CTT package from R \cite{ctt}. Furthermore, if Cronbach's alpha was lower than 0.7 --- indicating a reliability classified by \citeA{devellis} and \citeA{toetsen} as questionnable at best---, the items indicated by the package as the most weak items are omitted until the alpha is higher than 0.7 as described by \citeA{toetsen} or participants are omitted since all of the items they responded to are omitted.

\subsection{Item response theory}

In order to provide an alternative measure of reliability, the EAP score from the TAM package in R \cite{tamr} is calculated, based on item response theory (IRT). This serves the determination of the reliability of the tests and the reliability of the questionnaire. Additionally, this package calculates the person ability score ($\theta$'s) and the item difficulties as dependent items, in contrast to classical test theory which calculates both values independently. The 1 parameter logistics IRT model or the Rasch model is used to determine these variables over the 2PL or higher models, since the sample is expected to be too small to provide any reliable estimators for other variables than item difficulty (e.g. item discrimination). Finally, the TAM package supports defining fixed item difficulties, which is necessary in order to compare two sets of test scores. The item difficulties from the combined pretest matrices are used for setting the item difficulties when determining the IRT person abilities of the posttest matrices.

\subsection{Descriptive statistics}

For each category described above, the descriptive statistics will be provided as sample size, minimum value, maximum value, mean value, variance, the skew, and the kurtosis (provided by the describe() function from the scipy.stats package for python), the t- and p-values for the normality of the distribution (provided by the normality() function from the same package), and the reliability from either classical test theory or item response theory as described in the previous sections.

\subsection{Comparison tests}

The tests used for comparing the control and experimental group are the Mann-Whitney U test and Welch's t-test provided by the scipy.stats package for python. If the p-value for the normality test is significant ($p<0.05$) for both distributions, the parametric Welch's t-test is used for the interpretation, otherwise the non-parametric Mann-Whitney U test is used.

\subsection{Interviews}
