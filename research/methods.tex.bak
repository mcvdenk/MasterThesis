\chapter{Methods}
\label{ch:methods}

This research aims to answer the research questions stated within the \nameref{sec:intro_evaluation} section on page~\pageref{sec:intro_evaluation} by the methods described within this chapter.

\section{Research design}
\label{sec:researchdesign}

Research questions~\ref{benefit}\ref{effectiveness}, \ref{efficiency}, \ref{perception}\ref{usefulness} and \ref{ease} are investigated using intervention-based research. The respondents are individually assigned to a condition, since this will provide the most valid and reliable results. Additionally, research questions~\ref{usefulness} and \ref{ease} will also be investigated using open questions on the questionnaire form and by conducting interviews with a sample of the participants.

The quantitative and qualitative results will be mixed for the purposes of triangulation and expansion as described by \citeA{mixedmethods}. The interviews and logs could provide insight in the degree of which the systems were used the intended way and in why students had certain perceptions on using the systems. Both triangulation and expansion will be on a partial level of mixing, will take place concurrently, and the quantitative data will be dominant, since the qualitative data exists only to triangulate and expand the quantitative data. 

\section{Respondents}
\label{sec:respondents}

\paragraph{Approach} 100 15 to 17 year old tenth grade Dutch high school students will be approached by giving a presentation within one of the regular lessons. Since these student have to prepare for a school test related to the subject matter taught by the system, the students are already incentivised to use the system. Furthermore, the students will be rewarded with a \euro{} 5 voucher for participation in order to increase the response rate. Within the existing school curriculum, there are already two instructions planned for the topic of Dutch renaissance literature. At the end of the instruction, the researcher introduces the experiment and the system to the participants. This is meant both to attract students to participate as well as to provide a briefing in verbal form next to the written form. Within the presentation, the benefits are stated (better preparation for the exam, preview for the exam), it is stressed that participation is voluntary and that the data will be collected anonymously, the informed consent form is introduced, and finally the reward (icecream vourchers) is announced, together with the conditions for receiving the reward. The information provided on the seperate experimental conditions is limited to there being two different versions, without elaborating what these versions entail. This is in order to prevent prejudgements from within the user, making it a double-blind experiment. It could however happen that the participants learn about each others version during the experiment, which unfortunately cannot be prevented.

\paragraph{Informed consent form} The informed consent form, included within the \nameref{app:consentform} appendix on page~\pageref{app:consentform}, contains a letter, the written briefing, and a form which has to be signed by both the parent or caretaker and the participant. It also contains a code for verifying that a user within the system did indeed sign this form. The briefing contains a description of the research, the advantages of participation, and the procedure of the experiment.

\paragraph{Division of respondents} Users are assigned alternately to the control group or the experimental group. This pseudo-random assignment increases the validity of the experimental design without risking one group becoming larger than the other group. This however only holds up for the initial group, because dropout rates between the group could vary resulting in differently sized finished groups.

\paragraph{Selection of respondents} Finally, only after using the system for at least 15 minutes for 6 days the respondents were prompted with the posttest and questionnaire, reducing the number of respondents to only those with reasonable benefit from and experience with the system. The 15 minutes are estimated to be the amount of time necessary to review one section in the instructional material, and the 6 days are chosen as a balance between having covered a large enough portion of the material to measure a significant learning gain without the participant investment being too large resulting in no student wanting to participate. The 6 days of learning ideally take place subsequently, since then students have a higher chance of retrieving repeating instances correctly. However, it is likely that participants could forget about the experiment or be too busy to invest the 15 minutes, and therefore they are allowed one nonactive day during the experiment. 

\section{Instrumentation}
\label{sec:instrumentation}

\subsection{Test}

In order to assess what the student learned over the course of using the software, the participants are tested before and after the experiment. Each test consists out of a knowledge section and a comprehension section, derived from Bloom's Taxonomy \cite{bloom}. The knowledge section aimed at measuring whether the rote memorisation was effective, and the comprehension section served the purpose of measuring whether the explicit relations within the flashmaps scaffolded the comprehension. Only these two levels were chosen, since higher levels are more time consuming to create and to answer by the students, and the first step is to only measure whether students can already generalise from only rehearsing questions on the knowledge level to questions on the comprehension level.

The itembank used for the knowledge questions were the flashcards themselves, and the itembank for the comprehension questions a seperately constructed set of 10 items. Both the knowledge questions and the comprehension questions are based on a concept map encompassing the instructional material created by the researcher. The pretest and the posttest both randomly select 5 questions from each bank with non-overlapping items. By randomly selecting the questions, the overall knowledge and comprehension are measured instead of specific subsets measured seperately on the pretest and posttest. This eliminates the specific item difficulty variable from the learning gain, increasing its validity. The random variable however also increases the variability of scores, decreasing the significance when comparing the conditions.

Finally, a rubic is created in order to quantisise the openly formulated answers, consisting out of possible answer categories for each item. In order to verify whether the rubics is a reliable measure for quantificating the open answers on the test questions, first the inter-reliability is calculated. This is done by calculating both the proportionate agreement as the more reliable weighted Cohen's kappa \cite{kappa} between of one of the teachers and the researcher which marked 10 randomly seleceted knowledge items and 12 randomly selected comprehension items. The items were selected from both the pretest and posttest from random participants, and shuffled in order to minimise any halo effect. As can be seen in table~\ref{tab:irr}, Cohen's Kappa is 0.800 --- with 1 response option indicated as met by the teacher and not met by the researcher and 3 response options indicated as vice versa ---, which is generally seen as good \cite{baarda}. However, it is relatively low in comparison to the proportional agreement of 0.944. This can be attributed to relatively few response options being indicated as met in general and the Cohen's Kappa having a base assumption of purely chance-based agreement. The rubic is included within the \nameref{app:instruments} appendix on page~\pageref{sec:rubic}

\begin{table}
    \centering
    \begin{tabular}{lr}
        Amount of rated flashcards         & 10 \\
        Amount of rated items              & 12 \\
        Granted by both                    & 10 \\
        Only granted by the teacher        &  1 \\
        Only granted by the researcher     &  3 \\
        Granted by none                    & 58 \\
        Total amount of possible responses & 72 \\
        Proportionate agreement            & 0.9444 \\
        Cohen's kappa                      & 0.8003 \\
    \end{tabular}
    \caption{Statistics for calculating the inter-rater reliability}
    \label{tab:irr}
\end{table}

\subsection{Questionnaire}

After the respondent filled in the second test (or posttest), a questionnaire is prompted. The questionnaire consists out of translated items to Dutch from the Technology Acceptance Model (TAM) \cite{tam}, containing items measuring perceived usefulness and perceived ease of use. \citeA{devellis} describes that the validity of a questionnaire can be increased by formulating the items mixed positively and negatively, and by repeating the items. Therefore, the items from TAM are translated to Dutch in both a positive and a negative phrasing. For each participant, 2 sets of items are created per TAM category: for each item, a phrasing is selected at random after which the set is shuffled; another set of shuffled perceived usefulness items is created with each item having the opposite phrasing of its counterpart in set 1. This results in 4 sets of items encompassing both phrasings of all items. For each item, the participant can indicate whether he completely disagrees (-2), he disagrees (-1), he neither agrees nor disagrees (0), he agrees (1), or he completely agrees (2). The translated questions are included within the \nameref{app:instruments} appendix on page~\pageref{sec:tamquestions}.

Additionally, the form contains two open questions, namely to describe what the participant thought was good about the system, and what he thought could be improved. Finally, he can fill in his email address if he is interested in being interviewed afterwards.

\subsection{Data collection during experiment}

The variables described below are extracted from the database in order to provide statistics about the use of the system.

\paragraph{Correctness retrievals} For each response to a flashcard or flashmap, information is stored about whether the instance was retrieved correctly by the participant. This is not only useful for the rescheduling of the instance, but also in order to gain more insights in how often a participant was able to retrieve instances correctly, and to verify whether the success rate is indeed around 90\% as stated by \citeA{microlearning}.

\paragraph{Retrieval time} Furthermore, the start time (when an instance is sent from the server to the participant) and the end time (when the response was received by the server) are stored for each response in order to measure how much time was spent on each instance, and on the system in general.

\paragraph{Active days} Finally, a list is stored for each participant containing the dates that the participant was active. An active day is here defined as a day where the participant is active for 15 minutes or where the participant reviewed all of the available instances within the read sections. This data is useful for the system to know when to prompt the posttest, but also serves as a statistic on participation rates.

\subsection{Interview}

The respondents who filled in their email address at the end of the questionnaire form are finally interviewed. The interview is open instead of structured or semi-structured, since the interview should provide the interviewees the opportunity to tell about their own experiences from using the system, and since the data covering specific questions is already gathered within the usage data, the questionnaire and the two open questions. The researcher will use topics based on TAM in order to verify during the interview whether the main area is covered, with an open question at the end asking for general possible improvements. Furthermore, the interview will take place as one group interview, since the school organisation only has limited availability options for providing seperate rooms fit for interviews and the group interview being the most efficient option. At the start of the interview, the interviewees will be disclosed about the aims of the research in order to establish the different versions. Results will however not be disclosed until after the experiment in order to leave the interviewees unbiased in their opinion about the perceived usefulness. Finally, the interview is recorded for later analysis with the interviewees' permission.

\section{Procedure}
\label{sec:procedure}

\paragraph{Review concept map, flashcards, and item bank} In order to verify whether the content offered within the system is in alignment with the learning goals set by the teachers, one of the teachers is asked to review the content. The feedback received from the teacher is afterwards incorporated by altering the dataset. This can be seen as the focus evaluation of the product \cite{slo}.

\paragraph{Approval ethical committee} Before the actual experiment can take place, the research setup first has to be approved by the ethical committee of the University of Twente. This takes place before the system is introduced to the students.

\paragraph{Lessons} Concurrent with the teacher reviewing the material and the ethical committee reviewing the methodology, the two regularly planned lessons covering the material take place.

\paragraph{Presentation} At the end of the second lesson, the researcher introduces the system to the students.

\paragraph{Briefing and Informed Consent form} The students receive a written version of the briefing after the presentation together with an informed consent form, which they have to sign as well as one of their parents or caretakers before they can partake in the experiment.

\paragraph{Assignment conditions} At the beginning of the experiment, the students are assigned to either the control or the experimental condition pseudo-randomly by the software.

\paragraph{Descriptives} Before the experiment itself starts, the gender and the birthdate of the participants are prompted. This provides descriptive statistics necessary for the measure of generalisability of the results.

\paragraph{Pretest} Another form prompted towards the user before the start of the experiment is the pretest, measuring how much the user already knows and understands about the subject. This test is elaborated within the next section.

\paragraph{Experiment} The experiment itself consists of the participants having to review instances for 15 minutes over the course of 6 days. Each session consists of being presented by questions or incomplete concept maps, trying to retrieve the correct answer or missing concepts from memory, and indicating whether the correct answer or missing concept was successfully retrieved.

\paragraph{Posttest} The seventh day the participant logs in to the system, the posttest is prompted, measuring the knowledge and understanding of the respondent after the experiment. This test uses the same itembank as the pretest, and is thereby also elaborated within the next section.

\paragraph{Questionnaire} After filling in the posttest, the participant is also asked to fill in a questionnaire in order to measure how the student experienced the software. 

\paragraph{Debriefing} Finally, the participant is presented with the debriefing text from figure~\ref{fig:ui_debriefing} on page~\pageref{fig:ui_debriefing}. This states that the user will soon receive the reward, that he is allowed to keep using the system, and that he can contact the researcher if he has questions or when he wants to see his personal data. The respondent is now finished with the main experiment.

\paragraph{Interview} Those who volunteered for the interview will be sent an invitation by email for the open group interview.

\paragraph{Rewards} The vouchers and the list containing the names of students having participated within the experiment will be handed over to the teachers after the scoring of the test administered within the school in order to avoid influencing the teachers during the marking of the tests.

\section{Analysis}
\label{sec:analysis}

\subsection{Test scores}

In order to determine the score on a test, an item matrix is created with the response categories from the rubic as columns, the participants as rows, and the cells containing a 0 or 1 indicating whether the participants answer contained the response category. Each possible answer category from the rubic is chosen for the columns over the test items in order to more accurately determine item difficulties. The raw item difficulties are then calculated by the sum over the columns, whereas the raw person abilities are determined by the sum over the rows. Furthermore, the absolute and relative learning gain are calculated for comparing the conditions. The absolute learning gain entails simply subtracting the pretest person abilities from the posttest person abilities, resulting in a list of scores indicating how many more points each person scored on the posttest than on the pretest. The relative learning gain is calculated by the posttest score minus the pretest score divided by the maximum possible posttest score minus the pretest score. The main advantage of using relative gain over absolute gain is that each user can have a different maximum test score because of the items having different maximum scores and them being randomly selected for either the pre- or posttest, and the relative learning gain implicitly controls for this where the absolute gain does not. Therefore, the relative learning gain is the score used for comparison of the control group and experimental group.

For each subcategory, two histograms are plotted: one for the item difficulty (expressed as the average amount of correct answer on an item per person) and one for the person ability (expressed as the average score of a person divided by the amount of items). The correction is necessary in order to compare the two groups on an equal level, and the numbers are not indicatory of the percentage of correct answers on the test. Both the pretest and the posttest histograms are plotted in one graph for each condition, and the learning gains of both conditions are also plotted in one graph for visualising the differences.

\subsection{Participation statistics}

For determining the participation rates, a matrix is constructed containing the different days of the experiment as columns, the participants as rows, and the cells containing a 0 or 1 indicating the participant being active on that day. The participation rates are then calculated by the sum over the rows, and the amount of active participants from the sum over the columns. These matrixes are constructed seperately for the control group, the experimental group, and the group of combined experimental conditions. The statistics are depicted as histograms for the amount of active days for a participant, and as a scatterdiagram showing the amount of activity for each day of the experiment, combined with a step diagram showing the cumulative amount of finished participants. The days are indicated by a number from 0 to 28, where 0 is the day of presentation, day 21 is the day of the school test, and 28 is the last day with any activity.

\subsection{Instance statistics}

This category contains all statistics derived from the Instance and Response objects in the database collected during the experiment. For all of the subcategories described in the paragraphs below, item matrices are constructed for the control group, experimental group, and the group of participants from combined conditions. These matrices consist of the Flashcards or Edges in the columns, depending on the condition, and the participants in the columns. Within the combined conditions group, flashcards are mapped to the first edge in their list of sources. Furthermore, for in the number of responses and time spent categories, the double flashmap instances are removed. These are the instances presented together in one flashmap to the user. Like in the previous categories, item scores are again calculated by sum over the columns, and person scores by sum over the rows. Additionally, a relative score is calculated by dividing sum by the amount of entries: for the item score the relative score is the sum (absolute score) divided by the amount of participants, and for the person score it is the absolute score divided by the amount of items. Finally, mean scores are calculated for both rows and columns. The relative score is meant for being able to compare both groups, whereas the mean score provides additional insight in the score per reviewed instance. Finally, histograms are depicted for both the item scores as the user scores.

\paragraph{Reviewed instances} In order to determine how much of the instructional material is reviewed by each participant, the first subcategory investigated is the amount of reviewed instances. The cell of the item matrices contains whether an instance for a participant contains any response. The relative score proves here the most useful, since this is the ratio of covered material.

\paragraph{Number of responses} This statistic contains information on the number of times an instance is retrieved by a participant, which is included in order to compare whether both groups had an equal amount of opportunities to rehearse the instances. Since the ratio of retrieved instances is already known from the previous category, the mean number of responses per instance is the most useful score for comparing the experimental groups.

\paragraph{Exponents} The exponent statistic indicates the amount of responses since the last incorrect retrieval. This is included as a measure of learning progress of an instance. For the same reasons as for the previous category, the mean values are the most useful measures for comparing the groups.

\paragraph{Responses marked as correct} The ratio of the number of correct retrievals and the total number of retrievals is included in order to establish whether both groups had the same difficulty of retrieval from memory, and to verify whether participants indeed retrieve the instance correctly for 90\% of the retrievals as indicated by \citeA{microlearning}. The only meaningful values are here the mean values.

\paragraph{Time spent} Finally, the amount of time spent is calculated for each instance by the sum the retrieval time of each response, where the retrieval time is calculated by subtracting the time the instance was sent to the client from the time the instance was received on the server. The reason why this is chosen as the heuristic for time measurement is that this is the time spent on actual retrieval, whereas the times in between measure only the validating of an retieved answer or an intermission. Furthermore, the responses are filtered on only having retrieval times shorter than 5 minutes, since any longer retrieval time probably indicates an intermission between learning. For the same reason, the session time is also an unreliable measure. The absolute and mean values are here the most insightful, since the absolute values provide a measure of how much time the average user spent in total on the system, and the mean values are useful for comparing an individual flashcard with a flashmap. 

\subsection{Questionnaire scores}

The last category of quantitative variables contains the scores on the questionnaire, divided in the perceived usefulness score and the perceived ease of use score. Again, an item matrix is constructed, with the questionnaire items (either usefulness or ease of use) represented by the columns and the participants by the rows, and the cells containing how the participant rated the item. This score is determined by the rating from the positive phrasing minus the rating from the negative rating, which should result in twice the positive rating if the participant rated the negatively phrased item with the inverse rating of the positive phrased item. This result thereby ranges from -4 to 4.

\subsection{Classical test theory}

In order to establish Cronbach's alpha, classical test analyses are conducted over all of the above described statistics, using the CTT package from R \cite{ctt}. Furthermore, if Cronbach's alpha was lower than 0.7 --- indicating a reliability classified by \citeA{devellis} and \citeA{toetsen} as questionnable at best---, the items indicated by the package as the most weak items are omitted until the alpha is higher than 0.7 or participants are omitted since all of the items they responded to are omitted.

\subsection{Item response theory}

In order to provide an alternative measure of reliability, the EAP score from the TAM package in R \cite{tamr} is calculated, based on item response theory (IRT). This serves the determination of the reliability of the tests and the reliability of the questionnaire. Additionally, this package calculates the person ability score ($\theta$'s) and the item difficulties as dependent items, in contrast to classical test theory which calculates both values independently. The 1 parameter logistics IRT model or the Rasch model is used to determine these variables over the 2PL or higher models, since the sample is expected to be too small to provide any reliable estimators for other variables than item difficulty (e.g. item discrimination). Finally, the TAM package supports defining fixed item difficulties, which is necessary in order to compare two sets of test scores. The item difficulties from the combined pretest matrices are used for setting the item difficulties when determining the IRT person abilities of the posttest matrices.

\subsection{Descriptive statistics}

For each category described above, the descriptive statistics will be provided as sample size, minimum value, maximum value, mean value, variance, the skew, and the kurtosis, the t- and p-values for the normality of the distribution \cite{normality}, and the reliability from either classical test theory or item response theory as described in the previous sections.

\subsection{Comparison tests}

The tests used for comparing the control and experimental group are the Mann-Whitney U \cite{mannwhitneyu} test and Welch's t-test \cite{welch}. If the p-value for the normality test is significant ($p<0.05$) for both distributions, the parametric Welch's t-test is used for the interpretation, otherwise the non-parametric Mann-Whitney U test is used.

\subsection{Interviews}

Since there is only one group interview to be analysed it is not deemed necessary to transcribe and encode the interview data \cite{baarda}. Instead, only the relevant quotes are extracted and transcribed together with the time codes and added to the corresponding results within the next chapter.
